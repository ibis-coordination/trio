services:
  trio:
    build: .
    ports:
      - "8001:8000"
    environment:
      - TRIO_BACKEND_URL=http://litellm:4000
    volumes:
      - ./static:/app/static:ro
    depends_on:
      - litellm
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M

  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    # Internal only - no port exposed externally
    volumes:
      - ./litellm-config.yaml:/app/config.yaml
    environment:
      # Pass through API keys from host environment
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    command: ["--config", "/app/config.yaml"]
    depends_on:
      - ollama
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M

  ollama:
    image: ollama/ollama:latest
    # Internal only - no port exposed externally
    volumes:
      - harmonicteam_ollama_data:/root/.ollama
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G

volumes:
  harmonicteam_ollama_data:
    external: true
